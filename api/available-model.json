{
    "available-model": [
        {
            "name": "LinearRegression",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "fit_intercept", "default": "True", "dtype": "bool",
                "comment": "Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered)."},
                {"name": "positive", "default": "False", "dtype": "bool", "comment": "When set to True, forces the coefficients to be positive. This option is only supported for dense arrays."},
                {"name": "copy_X", "default": "True", "dtype": "bool", "comment": "If True, X will be copied; else, it may be overwritten."},
                {"name": "n_jobs", "default": "null", "dtype": "int", "comment": "The number of jobs to use for the computation. This will only provide speedup in case of sufficiently large problems, that is if firstly n_targets > 1 and secondly X is sparse or if positive is set to True. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details."}
            ],
            "return": [
                {"name": "coef_", "dtype": "array", "comment": "Coefficients of the features."},
                {"name": "intercept_", "dtype": "float", "comment": "Intercept (bias) added to the decision function."}
            ]
        },
        {
            "name": "LogisticRegression",
            "task": "Classification",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "C", "default": "1.0", "dtype": "float", "comment": "Inverse of regularization strength; must be a positive float. Smaller values specify stronger regularization."},
                {"name": "penalty", "default": "l2", "dtype": "str", "comment": "Specify the norm of the penalty. 'l2': L2 penalty term (default), 'l1': L1 penalty term, 'elasticnet': both L1 and L2 penalty terms. 'None': no penalty added. Deprecated since version 1.2; use None instead."},
                {"name": "dual", "default": "False", "dtype": "bool"},
                {"name": "tol", "default": "1e-4", "dtype": "float"},
                {"name": "intercept_scaling", "default": "1", "dtype": "float"},
                {"name": "class_weight", "default": "None", "dtype": "dict or 'balanced'"},
                {"name": "fit_intercept", "default": "True", "dtype": "bool", "comment": "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function."},
                {"name": "random_state", "default": "None", "dtype": "int or RandomState instance"},
                {"name": "solver", "default": "lbfgs", "dtype": "str"},
                {"name": "max_iter", "default": "100", "dtype": "int", "comment": "Maximum number of iterations taken for the solvers to converge."},
                {"name": "multi_class", "default": "'auto'", "dtype": "'auto', 'ovr', 'multinomial'", "comment": "If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimized is the multinomial loss fit across the entire probability distribution."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "For the liblinear and lbfgs solvers set verbose to any positive number for verbosity."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit as initialization."},
                {"name": "n_jobs", "default": "None", "dtype": "int", "comment": "Number of CPU cores used when parallelizing over classes if multi_class='ovr'."},
                {"name": "l1_ratio", "default": "None", "dtype": "float", "comment": "The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if penalty='elasticnet'."}
                
            ],
            "return": [
                {"name": "coef_", "dtype": "array", "comment": "Coefficient of the features."},
                {"name": "intercept_", "dtype": "float", "comment": "Intercept (bias) added to the decision function."}
            ]
        },
        {
            "name": "RidgeRegression",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "alpha", "default": "1.0", "dtype": "float", "comment": "Constant that multiplies the L2 term, controlling regularization strength. Must be a non-negative float."},
                {"name": "fit_intercept", "default": "True", "dtype": "bool", "comment": "Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations (i.e., X and y are expected to be centered)."},
                {"name": "solver", "default": "auto", "dtype": "str", "comment": "Solver to use in the computational routines. 'auto' chooses the solver automatically based on the type of data."},
                {"name": "copy_X", "default": "True", "dtype": "bool", "comment": "If True, X will be copied; else, it may be overwritten."},
                {"name": "max_iter", "default": "null", "dtype": "int", "comment": "Maximum number of iterations for the conjugate gradient solver. Default values depend on the solver."},
                {"name": "tol", "default": "1e-4", "dtype": "float", "comment": "The precision of the solution is determined by tol, which specifies a different convergence criterion for each solver."},
                {"name": "positive", "default": "False", "dtype": "bool", "comment": "When set to True, forces the coefficients to be positive. Only 'lbfgs' solver is supported in this case."},
                {"name": "random_state", "default": "null", "dtype": "int or RandomState instance", "comment": "Used when solver == 'sag' or 'saga' to shuffle the data."}
            ],
            "return": [
                {"name": "coef_", "dtype": "array", "comment": "Weight vector(s)."},
                {"name": "intercept_", "dtype": "float or ndarray of shape (n_targets,)", "comment": "Independent term in the decision function. Set to 0.0 if fit_intercept = False."},
                {"name": "n_iter", "dtype": "None or ndarray of shape (n_targets,)", "comment": "Actual number of iterations for each target. Available only for 'sag' and 'lsqr' solvers. Other solvers will return None. New in version 0.17."},
                {"name": "n_features_in", "dtype": "int", "comment": "Number of features seen during fit. New in version 0.24."},
                {"name": "feature_names_in", "dtype": "ndarray of shape (n_features_in_,)", "comment": "Array of feature names, indicating the order of features in the training data. New in version 0.24."}
            ]
        },
        {
            "name": "LassoRegression",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "alpha", "default": "1.0", "dtype": "float", "comment": "Constant that multiplies the L1 term, controlling regularization strength. Must be a non-negative float."},
                {"name": "fit_intercept", "default": "True", "dtype": "bool", "comment": "Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e., data is expected to be centered)."},
                {"name": "precompute", "default": "False", "dtype": "bool or array-like of shape (n_features, n_features)", "comment": "Whether to use a precomputed Gram matrix to speed up calculations. For sparse input, this option is always False to preserve sparsity."},
                {"name": "copy_X", "default": "True", "dtype": "bool", "comment": "If True, X will be copied; else, it may be overwritten."},
                {"name": "max_iter", "default": "1000", "dtype": "int", "comment": "The maximum number of iterations."},
                {"name": "tol", "default": "1e-4", "dtype": "float", "comment": "The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See the Glossary."},
                {"name": "positive", "default": "False", "dtype": "bool", "comment": "When set to True, forces the coefficients to be positive."},
                {"name": "random_state", "default": "null", "dtype": "int or RandomState instance", "comment": "The seed of the pseudo-random number generator that selects a random feature to update. Used when selection == 'random'. Pass an int for reproducible output across multiple function calls. See Glossary."},
                {"name": "selection", "default": "cyclic", "dtype": "str ", "comment": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default."}
            ],
            "return": [
                {"name": "coef_", "dtype": "array", "comment": "Parameter vector (w in the cost function formula)."},
                {"name": "dual_gap", "dtype": "float or ndarray of shape (n_targets,)", "comment": "Given param alpha, the dual gaps at the end of the optimization, same shape as each observation of y."},
                {"name": "sparse_coef", "dtype": "sparse matrix of shape (n_features, 1) or (n_targets, n_features)", "comment": "Sparse representation of the fitted coef_."},
                {"name": "intercept_", "dtype": "float or ndarray of shape (n_targets,)", "comment": "Independent term in the decision function."},
                {"name": "n_iter", "dtype": "int or list of int", "comment": "Number of iterations run by the coordinate descent solver to reach the specified tolerance."},
                {"name": "n_features_in", "dtype": "int", "comment": "Number of features seen during fit. New in version 0.24."},
                {"name": "feature_names_in", "dtype": "ndarray of shape (n_features_in_,)", "comment": "Names of features seen during fit. Defined only when X has feature names that are all strings. New in version 0.24."}
            ]
        },
        {
            "name": "Svm",
            "task": "Both",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "C", "default": "1.0", "dtype": "float", "comment": "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared L2 penalty."},
                {"name": "kernel", "default": "rbf", "dtype": "str", "comment": "Specifies the kernel type to be used in the algorithm. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices."},
                {"name": "gamma", "default": "scale", "dtype": "float", "comment": "Kernel coefficient for ‘rbf’, ‘poly’, and ‘sigmoid’. If gamma='scale' (default) is passed, then it uses 1 / (n_features * X.var()) as the value of gamma."},
                {"name": "degree", "default":" 3", "dtype": "int", "comment": "Degree of the polynomial kernel function (‘poly’). Must be non-negative. Ignored by all other kernels."},
                {"name": "coef", "default": "0.0", "dtype": "float", "comment": "Independent term in the kernel function. It is only significant in ‘poly’ and ‘sigmoid’."},
                {"name": "shrinking", "default": "True", "dtype": "bool", "comment": "Whether to use the shrinking heuristic."},
                {"name": "probability", "default": "False", "dtype": "bool", "comment": "Whether to enable probability estimates. This must be enabled prior to calling fit, will slow down that method as it internally uses 5-fold cross-validation."},
                {"name": "tol", "default": "1e-3", "dtype": "float", "comment": "Tolerance for stopping criterion."},
                {"name": "cache_size", "default": "200", "dtype": "float", "comment": "Specify the size of the kernel cache (in MB)."},
                {"name": "class_weight", "default": "None", "dtype": "dict or ‘balanced’", "comment": "Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one."},
                {"name": "verbose", "default": "False", "dtype": "bool", "comment": "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm."},
                {"name": "max_iter", "default": "-1", "dtype": "int", "comment": "Hard limit on iterations within solver, or -1 for no limit."},
                {"name": "decision_function_shape", "default": "ovr", "dtype": "str ", "comment": "Whether to return a one-vs-rest (‘ovr’) decision function or the original one-vs-one (‘ovo’) decision function."}
            ],
            "return": [
                {"name": "support_", "dtype": "array", "comment": "Indices of support vectors."},
                {"name": "dual_coef_", "dtype": "array", "comment": "Dual coefficients of the support vector in the decision function, multiplied by their targets."},
                {"name": "intercept_", "dtype": "array", "comment": "Constants in the decision function."}
            ]
        },
        {
            "name": "DecisionTree",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "criterion", "default": "mse", "dtype": "str", "comment": "The function to measure the quality of a split."},
                {"name": "max_depth", "default": "None", "dtype": "int", "comment": "The maximum depth of the tree."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node."},
                {"name": "min_weight_fraction_leaf", "default": "0.0", "dtype": "float", "comment": "The minimum weighted fraction of the sum total of weights required to be at a leaf node."},
                {"name": "max_features", "default": "None", "dtype": "int, float or str", "comment": "The number of features to consider when looking for the best split."},
                {"name": "random_state", "default": "None", "dtype": "int, RandomState or None", "comment": "Controls the randomness of the estimator."},
                {"name": "min_impurity_decrease", "default": "0.0", "dtype": "float", "comment": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},
                {"name": "class_weight", "default": "None", "dtype": "dict", "comment": "Set the parameter C of class i to class_weight[i]*C for SVC."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "comment": "Complexity parameter used for Minimal Cost-Complexity Pruning."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "comment": "Array of predictions."},
                {"name": "predict_proba", "dtype": "array", "comment": "Array of class probabilities."},
                {"name": "max_leaf_nodes", "dtype": "int", "comment": "The maximum number of leaf nodes in best-first fashion."}
            ]
        },
        {
            "name": "DecisionTreeClassification",
            "task": "Classification",
            "data": "Supervised, binary data",
            "params": [
                {"name": "criterion", "default": "gini", "dtype": "str", "description": "The function to measure the quality of a split."},
                {"name": "splitter", "default": "best", "dtype": "str", "description": "The strategy used to choose the split at each node."},
                {"name": "max_depth", "default": "None", "dtype": "int", "description": "The maximum depth of the tree."},
                {"name": "min_samples_split", "default": "2", "dtype": "int", "description": "The minimum number of samples required to split an internal node."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "description": "The minimum number of samples required to be at a leaf node."},
                {"name": "min_weight_fraction_leaf", "default": "0.0", "dtype": "float", "description": "The minimum weighted fraction of the sum total of weights required to be at a leaf node."},
                {"name": "max_features", "default": "None", "dtype": "int or float", "description": "The number of features to consider when looking for the best split."},
                {"name": "random_state", "default": "None", "dtype": "int", "description": "Controls the randomness of the estimator."},
                {"name": "max_leaf_nodes", "default": "None", "dtype": "int", "description": "Grow a tree with max_leaf_nodes in best-first fashion."},
                {"name": "min_impurity_decrease", "default": "0.0", "dtype": "float", "description": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},
                {"name": "class_weight", "default": "None", "dtype": "dict, list of dict, or 'balanced'", "description": "Weights associated with classes."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "description": "Complexity parameter for Minimal Cost-Complexity Pruning."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "description": "Predicted class labels for each data point."},
                {"name": "predict_proba", "dtype": "array", "description": "Probability estimates of the classes."},
                {"name": "max_leaf_nodes", "dtype": "int", "description": "Inferred value of max_leaf_nodes."}
            ]
        },
        {
            "name": "RandomForest",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "comment": "The number of trees in the forest."},
                {"name": "max_depth", "default": "None", "dtype": "int", "comment": "The maximum depth of the tree."},
                {"name": "criterion", "default": "mse", "dtype": "str", "comment": "The function to measure the quality of a split."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node."},
                {"name": "min_weight_fraction_leaf", "default":" 0.0", "dtype": "float", "comment": "The minimum weighted fraction of the sum total of weights required to be at a leaf node."},
                {"name": "max_features", "default": "1.0", "dtype": "int or float", "comment": "The number of features to consider when looking for the best split."},
                {"name": "max_leaf_nodes", "default": "None", "dtype": "int", "comment": "Grow trees with max_leaf_nodes in best-first fashion."},
                {"name": "min_impurity_decrease", "default": "0.0", "dtype": "float", "comment": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},
                {"name": "bootstrap", "default": "True", "dtype": "bool", "comment": "Whether bootstrap samples are used when building trees."},
                {"name": "oob_score", "default": "False", "dtype": "bool", "comment": "Whether to use out-of-bag samples to estimate the generalization score."},
                {"name": "n_jobs", "default": "None", "dtype": "int", "comment": "The number of jobs to run in parallel."},
                {"name": "random_state", "default": "None", "dtype": "int", "comment": "Controls both the randomness of the bootstrapping of the samples and the sampling of the features."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "Controls the verbosity when fitting and predicting."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble."},
                {"name": "class_weight", "default": "None", "dtype": "dict", "comment": "Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "comment": "Complexity parameter used for Minimal Cost-Complexity Pruning."},
                {"name": "max_samples", "default": "None", "dtype": "int", "comment": "If bootstrap is True, the number of samples to draw from X to train each base estimator."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "comment": "The predicted values for each sample."},
                {"name": "predict_proba", "dtype": "array", "comment": "The class probabilities of the input samples."}
            ]
        },
        {
            "name": "RandomForestClassification",
            "task": "Classification",
            "data": "Supervised, binary data",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "comment": "The number of trees in the forest."},
                {"name": "max_depth", "default": "None", "dtype": "int", "comment": "The maximum depth of the tree."},
                {"name": "criterion", "default": "gini", "dtype": "str", "comment": "The function to measure the quality of a split."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node."},
                {"name": "min_weight_fraction_leaf", "default": "0.0", "dtype": "float", "comment": "The minimum weighted fraction of the sum total of weights required to be at a leaf node."},
                {"name": "max_features", "default": "sqrt", "dtype": "int or float", "comment": "The number of features to consider when looking for the best split."},
                {"name": "max_leaf_nodes", "default": "None", "dtype": "int", "comment": "Grow trees with max_leaf_nodes in best-first fashion."},
                {"name": "min_impurity_decrease", "default": "0.0", "dtype": "float", "comment": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},
                {"name": "bootstrap", "default": "True", "dtype": "bool", "comment": "Whether bootstrap samples are used when building trees."},
                {"name": "oob_score", "default": "False", "dtype": "bool", "comment": "Whether to use out-of-bag samples to estimate the generalization score."},
                {"name": "n_jobs", "default": "None", "dtype": "int", "comment": "The number of jobs to run in parallel."},
                {"name": "random_state", "default": "None", "dtype": "int", "comment": "Controls both the randomness of the bootstrapping of the samples and the sampling of the features."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "Controls the verbosity when fitting and predicting."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble."},
                {"name": "class_weight", "default": "None", "dtype": "dict", "comment": "Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "comment": "Complexity parameter used for Minimal Cost-Complexity Pruning."},
                {"name": "max_samples", "default": "None", "dtype": "int", "comment": "If bootstrap is True, the number of samples to draw from X to train each base estimator."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "comment": "The predicted values for each sample."},
                {"name": "predict_proba", "dtype": "array", "comment": "The class probabilities of the input samples."}
            ]
        },
        {
            "name": "KnnRegression",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "n_neighbors", "default": "5", "dtype": "int", "comment": "Number of neighbors to use by default for kneighbors queries."},
                {"name": "weights", "default": "uniform", "dtype": "str", "comment": "Weight function used in prediction. Possible values: 'uniform' or 'distance'."},
                {"name": "algorithm", "default": "auto", "dtype": "str", "comment": "Algorithm used to compute the nearest neighbors. Possible values: 'auto', 'ball_tree', 'kd_tree', 'brute'."},
                {"name": "leaf_size", "default": "30", "dtype": "int", "comment": "Leaf size passed to BallTree or KDTree."},
                {"name": "p", "default": "2", "dtype": "float", "comment": "Power parameter for the Minkowski metric. When p = 1, equivalent to using Manhattan distance, and Euclidean distance for p = 2."},
                {"name": "metric", "default": "minkowski", "dtype": "str or callable", "comment": "Metric to use for distance computation."},
                {"name": "metric_params", "default": "None", "dtype": "dict", "comment": "Additional keyword arguments for the metric function."},
                {"name": "n_jobs", "default": "None", "dtype": "int", "comment": "The number of parallel jobs to run for neighbors search."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "comment": "The predicted values for each sample."}
            ]
        },
        {
            "name": "KnnClassification",
            "task": "Classification",
            "data": "Supervised, binary data",
            "params": [
                {"name": "n_neighbors", "default": "5", "dtype": "int", "comment": "Number of neighbors to use by default for kneighbors queries."},
                {"name": "weights", "default": "uniform", "dtype": "str", "comment": "Weight function used in prediction. Possible values: 'uniform' or 'distance'."},
                {"name": "algorithm", "default": "auto", "dtype": "str", "comment": "Algorithm used to compute the nearest neighbors. Possible values: 'auto', 'ball_tree', 'kd_tree', 'brute'."},
                {"name": "leaf_size", "default": "30", "dtype": "int", "comment": "Leaf size passed to BallTree or KDTree."},
                {"name": "p", "default": "2", "dtype": "float", "comment": "Power parameter for the Minkowski metric. When p = 1, equivalent to using Manhattan distance, and Euclidean distance for p = 2."},
                {"name": "metric", "default": "minkowski", "dtype": "str or callable", "comment": "Metric to use for distance computation."},
                {"name": "metric_params", "default": "None", "dtype": "dict", "comment": "Additional keyword arguments for the metric function."},
                {"name": "n_jobs", "default": "None", "dtype": "int", "comment": "The number of parallel jobs to run for neighbors search."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "comment": "The predicted values for each sample."},
                {"name": "predict_proba", "dtype": "array", "comment": "Class probability estimates for the input samples."}
            ]
        },
        {
            "name": "KMeansClustering",
            "task": "Unsupervised",
            "data": "Numerical data",
            "params": [
                {"name": "n_clusters", "default": "8", "dtype": "int", "comment": "The number of clusters to form as well as the number of centroids to generate."},
                {"name": "init", "default": "k-means++", "dtype": "str or callable or array-like", "comment": "Method for initialization."},
                {"name": "n_init", "default": "10", "dtype": "'auto' or int", "comment": "Number of times the k-means algorithm is run with different centroid seeds."},
                {"name": "max_iter", "default":" 300", "dtype": "int", "comment": "Maximum number of iterations of the k-means algorithm for a single run."},
                {"name": "tol", "default": "1e-4", "dtype": "float", "comment": "Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "Verbosity mode."},
                {"name": "random_state", "default": "None", "dtype": "int, RandomState instance or None", "comment": "Determines random number generation for centroid initialization."},
                {"name": "copy_x", "default": "True", "dtype": "bool", "comment": "When pre-computing distances, it is more numerically accurate to center the data first."},
                {"name": "algorithm", "default": "lloyd", "dtype": "str", "comment": "K-means algorithm to use. Possible values: 'lloyd' or 'elkan'."}
            ],
            "return": [
                {"name": "labels_", "dtype": "array", "comment": "Labels of each point."},
                {"name": "cluster_centers_", "dtype": "array", "comment": "Coordinates of cluster centers."}
            ]
        },
        {
            "name": "NaïveBayesClassification",
            "task": "Classification",
            "data": "Supervised, binary or multiclass data",
            "params": [
                {"name": "alpha", "default": "1.0", "dtype": "float", "comment": "Smoothing parameter."},
                {"name": "priors", "default": "None", "dtype": "array-like of shape (n_classes,)", "comment": "Prior probabilities of the classes. If specified, the priors are not adjusted according to the data."},
                {"name": "var_smoothing", "default": "1e-9", "dtype": "float", "comment": "Portion of the largest variance of all features that is added to variances for calculation stability. New in version 0.20."}
            ],
            "return": [
                {"name": "class_log_prior_", "dtype": "array", "comment": "Log prior probabilities of classes."},
                {"name": "class_count_", "dtype": "array", "comment": "Number of training samples observed in each class."},
                {"name": "feature_count_", "dtype": "array", "comment": "Sum of feature occurrences over all classes."}
            ]
        },
        {
            "name": "GradientBoostingRegression",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "comment": "The number of boosting stages to perform. Values must be in the range [1, inf)."},
                {"name": "learning_rate", "default": "0.1", "dtype": "float", "comment": "Learning rate shrinks the contribution of each tree by learning_rate. Values must be in the range [0.0, inf)."},
                {"name": "max_depth", "default": "3", "dtype": "int or None", "comment": "Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If int, values must be in the range [1, inf)."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node. If int, values must be in the range [2, inf). If float, values must be in the range (0.0, 1.0] and min_samples_split will be ceil(min_samples_split * n_samples)."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0) and min_samples_leaf will be ceil(min_samples_leaf * n_samples)."},
                {"name": "max_features", "default": "None", "dtype": "int, float, or {'auto', 'sqrt', 'log2'}", "comment": "The number of features to consider when looking for the best split. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0] and the features considered at each split will be max(1, int(max_features * n_features_in_)). If 'sqrt', then max_features=sqrt(n_features). If 'log2', then max_features=log2(n_features). If None, then max_features=n_features."},
                {"name": "subsample", "default": "1.0", "dtype": "float", "comment": "The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. Values must be in the range (0.0, 1.0]."},
                {"name": "criterion", "default": "friedman_mse", "dtype": "{'friedman_mse', 'squared_error'}", "comment": "The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases. New in version 0.18."},
                {"name": "min_weight_fraction_leaf", "default": "0.0", "dtype": "float", "comment": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. Values must be in the range [0.0, 0.5]."},
                {"name": "init", "default": "None", "dtype": "estimator or 'zero'", "comment": "An estimator object that is used to compute the initial predictions. init has to provide fit and predict. If 'zero', the initial raw predictions are set to zero. By default a DummyEstimator is used, predicting either the average target value (for loss='squared_error'), or a quantile for the other losses."},
                {"name": "random_state", "default": "None", "dtype": "int, RandomState instance or None", "comment": "Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split. It also controls the random splitting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls. See Glossary."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "Enable verbose output. If 1 then it prints progress and performance once in a while. If greater than 1 then it prints progress and performance for every tree. Values must be in the range [0, inf)."},
                {"name": "max_leaf_nodes", "default": "None", "dtype": "int or None", "comment": "Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. Values must be in the range [2, inf). If None, then unlimited number of leaf nodes."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See the Glossary."},
                {"name": "validation_fraction", "default": "0.1", "dtype": "float", "comment": "The proportion of training data to set aside as validation set for early stopping. Values must be in the range (0.0, 1.0). Only used if n_iter_no_change is set to an integer. New in version 0.20."},
                {"name": "n_iter_no_change", "default": "None", "dtype": "int", "comment": "n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default, it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations. The split is stratified. Values must be in the range [1, inf). New in version 0.20."},
                {"name": "tol", "default": "1e-4", "dtype": "float", "comment": "Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations, the training stops. Values must be in the range [0.0, inf). New in version 0.20."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "comment": "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. Values must be in the range [0.0, inf). See Minimal Cost-Complexity Pruning for details. New in version 0.22."},
                {"name": "loss", "default": "squared_error", "dtype": "str", "comment": "Loss function to be optimized. 'squared_error' refers to the squared error for regression. 'absolute_error' refers to the absolute error of regression and is a robust loss function. 'huber' is a combination of the two. 'quantile' allows quantile regression (use alpha to specify the quantile)."}
            ],
            "return": [
                {"name": "estimators_", "dtype": "array of objects"},
                {"name": "feature_importances_", "dtype": "ndarray of shape (n_features,)"},
                {"name": "oob_improvement_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "oob_scores_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "oob_score_", "dtype": "float"},
                {"name": "train_score_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "init_estimator_", "dtype": "estimator"},
                {"name": "estimators_", "dtype": "ndarray of DecisionTreeRegressor of shape (n_estimators, 1)"},
                {"name": "n_estimators_", "dtype": "int"},
                {"name": "n_features_in_", "dtype": "int"},
                {"name": "feature_names_in_", "dtype": "ndarray of shape (n_features_in_,)"},
                {"name": "max_features_", "dtype": "int"}
            ]
        },
        {
            "name": "GradientBoostingClassification",
            "task": "Classification",
            "data": "Supervised, binary or multiclass data",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "comment": "The number of boosting stages to perform. Values must be in the range [1, inf)."},
                {"name": "learning_rate", "default": "0.1", "dtype": "float", "comment": "Learning rate shrinks the contribution of each tree by learning_rate. Values must be in the range [0.0, inf)."},
                {"name": "max_depth", "default": "3", "dtype": "int or None", "comment": "Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If int, values must be in the range [1, inf)."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node. If int, values must be in the range [2, inf). If float, values must be in the range (0.0, 1.0] and min_samples_split will be ceil(min_samples_split * n_samples)."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0) and min_samples_leaf will be ceil(min_samples_leaf * n_samples)."},
                {"name": "max_features", "default": "None", "dtype": "int, float, or {'auto', 'sqrt', 'log2'}", "comment": "The number of features to consider when looking for the best split. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0] and the features considered at each split will be max(1, int(max_features * n_features_in_)). If 'sqrt', then max_features=sqrt(n_features). If 'log2', then max_features=log2(n_features). If None, then max_features=n_features."},
                {"name": "subsample", "default": "1.0", "dtype": "float", "comment": "The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. Values must be in the range (0.0, 1.0]."},
                {"name": "criterion", "default": "friedman_mse", "dtype": "{'friedman_mse', 'squared_error'}", "comment": "The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases. New in version 0.18."},
                {"name": "min_weight_fraction_leaf", "default": "0.0", "dtype": "float", "comment": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. Values must be in the range [0.0, 0.5]."},
                {"name": "init", "default": "None", "dtype": "object or 'zero'", "comment": "An estimator object that is used to compute the initial predictions. init has to provide fit and predict_proba. If 'zero', the initial raw predictions are set to zero. By default, a DummyEstimator predicting the classes priors is used."},
                {"name": "random_state", "default": "None", "dtype": "int, RandomState instance or None", "comment": "Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split. It also controls the random splitting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls. See Glossary."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "Enable verbose output. If 1 then it prints progress and performance once in a while. If greater than 1 then it prints progress and performance for every tree. Values must be in the range [0, inf)."},
                {"name": "max_leaf_nodes", "default": "None", "dtype": "int or None", "comment": "Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. Values must be in the range [2, inf). If None, then unlimited number of leaf nodes."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See the Glossary."},
                {"name": "validation_fraction", "default": "0.1", "dtype": "float", "comment": "The proportion of training data to set aside as validation set for early stopping. Values must be in the range (0.0, 1.0). Only used if n_iter_no_change is set to an integer. New in version 0.20."},
                {"name": "n_iter_no_change", "default": "None", "dtype": "int", "comment": "n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default, it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations. The split is stratified. Values must be in the range [1, inf). New in version 0.20."},
                {"name": "tol", "default": "1e-4", "dtype": "float", "comment": "Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations, the training stops. Values must be in the range [0.0, inf). New in version 0.20."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "comment": "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. Values must be in the range [0.0, inf). See Minimal Cost-Complexity Pruning for details. New in version 0.22."},
                {"name": "loss", "default": "log_loss", "dtype": "str", "comment": "The loss function to be optimized. 'log_loss' refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss 'exponential', gradient boosting recovers the AdaBoost algorithm."}
            ],
            "return": [
                {"name": "estimators_", "dtype": "array of objects"},
                {"name": "feature_importances_", "dtype": "ndarray of shape (n_features,)"},
                {"name": "oob_improvement_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "oob_scores_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "oob_score_", "dtype": "float"},
                {"name": "train_score_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "init_estimator_", "dtype": "object"},
                {"name": "estimators_", "dtype": "ndarray of DecisionTreeRegressor of shape (n_estimators, loss_.K)"},
                {"name": "classes_", "dtype": "ndarray of shape (n_classes,)"},
                {"name": "n_features_in_", "dtype": "int"},
                {"name": "feature_names_in_", "dtype": "ndarray of shape (n_features_in_,)"},
                {"name": "n_classes_", "dtype": "int"},
                {"name": "max_features_", "dtype": "int"}
            ]
        },
        {
            "name": "XgboostRegression",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "comment": "The number of boosting stages to perform. Values must be in the range [1, inf)."},
                {"name": "learning_rate", "default": "0.1", "dtype": "float", "comment": "Learning rate shrinks the contribution of each tree by learning_rate. Values must be in the range [0.0, inf)."},
                {"name": "max_depth", "default": "6", "dtype": "int or None", "comment": "Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If int, values must be in the range [1, inf)."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node. If int, values must be in the range [2, inf). If float, values must be in the range (0.0, 1.0] and min_samples_split will be ceil(min_samples_split * n_samples)."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0) and min_samples_leaf will be ceil(min_samples_leaf * n_samples)."},
                {"name": "max_features", "default": "None", "dtype": "int, float, or {'auto', 'sqrt', 'log2'}", "comment": "The number of features to consider when looking for the best split. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0] and the features considered at each split will be max(1, int(max_features * n_features_in_)). If 'sqrt', then max_features=sqrt(n_features). If 'log2', then max_features=log2(n_features). If None, then max_features=n_features."}
            ],
            "return": [
                {"name": "Booster", "dtype": "object", "comment": "The trained booster model."}
            ]
        },
        {
            "name": "XgboostClassification",
            "task": "Classification",
            "data": "Supervised, binary or multiclass data",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "comment": "The number of boosting stages to perform. Values must be in the range [1, inf)."},
                {"name": "learning_rate", "default": "0.1", "dtype": "float", "comment": "Learning rate shrinks the contribution of each tree by learning_rate. Values must be in the range (0.0, inf)."},
                {"name": "max_depth", "default": "3", "dtype": "int or None", "comment": "Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If int, values must be in the range [1, inf)."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node. If int, values must be in the range [2, inf). If float, values must be in the range (0.0, 1.0] and min_samples_split will be ceil(min_samples_split * n_samples)."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0) and min_samples_leaf will be ceil(min_samples_leaf * n_samples)."},
                {"name": "max_features", "default": "None", "dtype": "int, float, or {'auto', 'sqrt', 'log2'}", "comment": "The number of features to consider when looking for the best split. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0] and the features considered at each split will be max(1, int(max_features * n_features_in_)). If 'sqrt', then max_features=sqrt(n_features). If 'log2', then max_features=log2(n_features). If None, then max_features=n_features."}
            ],
            "return": [
                {"name": "Booster", "dtype": "object", "comment": "The trained booster model."}
            ]
        },
        {
            "name": "DbscanClustering",
            "task": "Unsupervised",
            "data": "Numerical data",
            "params": [
                {"name": "eps", "default": "0.5", "dtype": "float", "comment": "The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function."},
                {"name": "min_samples", "default": "5", "dtype": "int", "comment": "The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. If min_samples is set to a higher value, DBSCAN will find denser clusters, whereas if it is set to a lower value, the found clusters will be more sparse."},
                {"name": "metric", "default": "euclidean", "dtype": "str", "comment": "The metric to use when calculating distance between instances in a feature array."},
                {"name": "metric_params", "default": "None", "dtype": "dict", "comment": "Additional keyword arguments for the metric function."},
                {"name": "algorithm", "default": "auto", "dtype": "str", "comment": "The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details."},
                {"name": "leaf_size", "default": "30", "dtype": "int", "comment": "Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem."},
                {"name": "p", "default": "None", "dtype": "float", "comment": "The power of the Minkowski metric to be used to calculate distance between points. If None, then p=2 (equivalent to the Euclidean distance)."},
                {"name": "n_jobs", "default": "None", "dtype": "int", "comment": "The number of parallel jobs to run. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors."}
            ],
            "return": [
                {"name": "labels_", "dtype": "array", "comment": "Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1."}
            ]
        },
        {
            "name": "BirchAlgorithm",
            "task": "Clustering",
            "data": "Unsupervised, numerical data",
            "params": [
                {"name": "threshold", "default": "0.5", "dtype": "float", "comment": "The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Setting this value to be very low promotes splitting and vice-versa."},
                {"name": "branching_factor", "default": "50", "dtype": "int", "comment": "Maximum number of CF subclusters in each node. If a new sample enters such that the number of subclusters exceeds the branching_factor, then that node is split into two nodes with the subclusters redistributed in each."},
                {"name": "clusters", "default": "30", "dtype": "int, None", "comment": "Number of clusters after the final clustering step, which treats the subclusters from the leaves as new samples. If None, the final clustering step is not performed, and the subclusters are returned as they are."},
                {"name": "compute_labels", "default": "True", "dtype": "bool", "comment": "Whether or not to compute labels for each fit."},
                {"name": "copy", "default": "True", "dtype": "bool", "comment": "Whether or not to make a copy of the given data. If set to False, the initial data will be overwritten."}
            ],
            "return": [
                {"name": "Set of clusters", "dtype": "dict"}
            ]
        },
        {
            "name": "IsolationForest",
            "task": "Clustering",
            "data": "Unsupervised, numerical and binary data",
            "description": "Isolation Forest is an ensemble algorithm for anomaly detection that isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "description": "The number of base estimators in the ensemble. Increasing the number of estimators will generally improve the performance but will also increase the computation time."},
                {"name": "max_samples", "default": "auto", "dtype": "int, float", "description": "The number of samples to draw from the input data to train each base estimator. If 'auto', then max_samples is set to the size of the input data. Increasing max_samples will result in a more robust model but may also increase the training time."},
                {"name": "contamination", "default": "auto", "dtype": "float or auto", "description": "The amount of contamination in the dataset. It represents the proportion of outliers in the dataset. If 'auto', it is set to the empirical contamination estimate, which is the proportion of samples that are outliers."},
                {"name": "max_features", "default": "1.0", "dtype": "float", "description": "The number of features to draw from the input data to train each base estimator. If 1.0, it uses all features for each base estimator. Smaller values may reduce overfitting."},
                {"name": "bootstrap", "default": "False", "dtype": "bool", "description": "If true, individual trees are fit on bootstrap samples of the input data. If false, the entire dataset is used to build each tree. Setting this parameter to true may improve performance."},
                {"name": "n_jobs", "default": "N    one", "dtype": "int", "description": "The number of parallel jobs to run. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. More jobs may result in faster execution but also increase memory consumption."},
                {"name": "random_state", "default": "None", "dtype": "int", "description": "Controls the randomness of the estimator. If int, random_state is the seed used by the random number generator; if None, the random number generator is the RandomState instance used by np.random."},
                {"name": "verbose", "default": "0", "dtype": "int", "description": "Controls the verbosity of the output during training. If 0, no output; if 1, some progress information; if greater than 1, the computation time for each tree is displayed."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "description": "If true, reuse the solution of the previous call to fit and add more estimators to the ensemble; otherwise, start with a new ensemble. Enabling warm_start can be useful for incremental training."}
            ],
            "return": [
                {"name": "selected", "dtype": "various parameters", "description": "The details of the selected features and splits for each base estimator in the ensemble."}
            ]
        }
        
    ]
}






