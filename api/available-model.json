{
    "available-model": [
        {
            "name": "LinearRegression",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "fit_intercept", "default": "True", "dtype": "bool", "comment": "Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered)."},
                {"name": "positive", "default": "False", "dtype": "bool", "comment": "When set to True, forces the coefficients to be positive. This option is only supported for dense arrays."},
                {"name": "copy_X", "default": "True", "dtype": "bool", "comment": "If True, X will be copied; else, it may be overwritten."},
                {"name": "n_jobs", "default": "-1", "dtype": "int", "comment": "The number of jobs to use for the computation. This will only provide speedup in case of sufficiently large problems, that is if firstly n_targets > 1 and secondly X is sparse or if positive is set to True. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details."}
            ],
            "return": [
                {"name": "coef_", "dtype": "array", "comment": "Coefficients of the features."},
                {"name": "intercept_", "dtype": "float", "comment": "Intercept (bias) added to the decision function."}
            ]
        },
        {
            "name": "Ridge",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "alpha", "default": "1.0", "dtype": "float", "comment": "Constant that multiplies the L2 term, controlling regularization strength. Must be a non-negative float."},
                {"name": "fit_intercept", "default": "True", "dtype": "bool", "comment": "Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations (i.e., X and y are expected to be centered)."},
                {"name": "solver", "default": "auto", "dtype": "str", "comment": "Solver to use in the computational routines. 'auto' chooses the solver automatically based on the type of data."},
                {"name": "copy_X", "default": "True", "dtype": "bool", "comment": "If True, X will be copied; else, it may be overwritten."},
                {"name": "max_iter", "default": "10", "dtype": "int", "comment": "Maximum number of iterations for the conjugate gradient solver. Default values depend on the solver."},
                {"name": "tol", "default": "0.0001", "dtype": "float", "comment": "The precision of the solution is determined by tol, which specifies a different convergence criterion for each solver."},
                {"name": "positive", "default": "False", "dtype": "bool", "comment": "When set to True, forces the coefficients to be positive. Only 'lbfgs' solver is supported in this case."},
                {"name": "random_state", "default": "null", "dtype": "int or RandomState instance", "comment": "Used when solver == 'sag' or 'saga' to shuffle the data."}
            ],
            "return": [
                {"name": "coef_", "dtype": "array", "comment": "Weight vector(s)."},
                {"name": "intercept_", "dtype": "float or ndarray of shape (n_targets,)", "comment": "Independent term in the decision function. Set to 0.0 if fit_intercept = False."},
                {"name": "n_iter", "dtype": "None or ndarray of shape (n_targets,)", "comment": "Actual number of iterations for each target. Available only for 'sag' and 'lsqr' solvers. Other solvers will return None. New in version 0.17."},
                {"name": "n_features_in", "dtype": "int", "comment": "Number of features seen during fit. New in version 0.24."},
                {"name": "feature_names_in", "dtype": "ndarray of shape (n_features_in_,)", "comment": "Array of feature names, indicating the order of features in the training data. New in version 0.24."}
            ]
        },
        {
            "name": "Lasso",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "alpha", "default": "1.0", "dtype": "float", "comment": "Constant that multiplies the L1 term, controlling regularization strength. Must be a non-negative float."},
                {"name": "fit_intercept", "default": "True", "dtype": "bool", "comment": "Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e., data is expected to be centered)."},
                {"name": "precompute", "default": "False", "dtype": "bool or array-like of shape (n_features, n_features)", "comment": "Whether to use a precomputed Gram matrix to speed up calculations. For sparse input, this option is always False to preserve sparsity."},
                {"name": "copy_X", "default": "True", "dtype": "bool", "comment": "If True, X will be copied; else, it may be overwritten."},
                {"name": "max_iter", "default": "1000", "dtype": "int", "comment": "The maximum number of iterations."},
                {"name": "tol", "default": "0.0001", "dtype": "float", "comment": "The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See the Glossary."},
                {"name": "positive", "default": "False", "dtype": "bool", "comment": "When set to True, forces the coefficients to be positive."},
                {"name": "random_state", "default": "null", "dtype": "int or RandomState instance", "comment": "The seed of the pseudo-random number generator that selects a random feature to update. Used when selection == 'random'. Pass an int for reproducible output across multiple function calls. See Glossary."},
                {"name": "selection", "default": "cyclic", "dtype": "str ", "comment": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default."}
            ],
            "return": [
                {"name": "coef_", "dtype": "array", "comment": "Parameter vector (w in the cost function formula)."},
                {"name": "dual_gap", "dtype": "float or ndarray of shape (n_targets,)", "comment": "Given param alpha, the dual gaps at the end of the optimization, same shape as each observation of y."},
                {"name": "sparse_coef", "dtype": "sparse matrix of shape (n_features, 1) or (n_targets, n_features)", "comment": "Sparse representation of the fitted coef_."},
                {"name": "intercept_", "dtype": "float or ndarray of shape (n_targets,)", "comment": "Independent term in the decision function."},
                {"name": "n_iter", "dtype": "int or list of int", "comment": "Number of iterations run by the coordinate descent solver to reach the specified tolerance."},
                {"name": "n_features_in", "dtype": "int", "comment": "Number of features seen during fit. New in version 0.24."},
                {"name": "feature_names_in", "dtype": "ndarray of shape (n_features_in_,)", "comment": "Names of features seen during fit. Defined only when X has feature names that are all strings. New in version 0.24."}
            ]
        },
        {
            "name": "SVR",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "kernel", "default": "'rbf'", "dtype": "str", "comment": "Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given, it is used to precompute the kernel matrix."},
                {"name": "degree", "default": "3", "dtype": "int","comment": "Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels."},
                {"name": "gamma", "default": "'scale'", "dtype": "str or float","comment": "Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. If 'gamma='scale'' (default) is passed, it uses 1 / (n_features * X.var()) as the value of gamma. If 'auto', uses 1 / n_features. If float, must be non-negative."},
                {"name": "coef0", "default": "0.0", "dtype": "float","comment": "Independent term in the kernel function. It is only significant in 'poly' and 'sigmoid'."},
                {"name": "tol", "default": "0.001", "dtype": "float", "comment": "Tolerance for stopping criterion."},
                {"name": "C", "default": "1.0", "dtype": "float","comment": "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty."},
                {"name": "epsilon", "default": "0.1", "dtype": "float", "comment": "Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. Must be non-negative."},
                {"name": "shrinking", "default": "True", "dtype": "bool", "comment": "Whether to use the shrinking heuristic."},
                {"name": "cache_size", "default": "200", "dtype": "float", "comment": "Specify the size of the kernel cache (in MB)."},
                {"name": "verbose", "default": "False", "dtype": "bool","comment": "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context."},
                {"name": "max_iter", "default": "-1", "dtype": "int","comment": "Hard limit on iterations within the solver, or -1 for no limit."}
            ],
            "return": [
                {"name": "coef_", "dtype": "array", "comment": "Coefficients of the features."},
                {"name": "intercept_", "dtype": "float", "comment": "Intercept (bias) added to the decision function."}
            ]
        },
        {
            "name": "DecisionTreeRegressor",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "criterion", "default": "mse", "dtype": "str", "comment": "The function to measure the quality of a split."},
                {"name": "max_depth", "default": "None", "dtype": "int", "comment": "The maximum depth of the tree."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node."},
                {"name": "min_weight_fraction_leaf", "default": "0.0", "dtype": "float", "comment": "The minimum weighted fraction of the sum total of weights required to be at a leaf node."},
                {"name": "max_features", "default": "None", "dtype": "int, float or str", "comment": "The number of features to consider when looking for the best split."},
                {"name": "random_state", "default": "None", "dtype": "int, RandomState or None", "comment": "Controls the randomness of the estimator."},
                {"name": "min_impurity_decrease", "default": "0.0", "dtype": "float", "comment": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "comment": "Complexity parameter used for Minimal Cost-Complexity Pruning."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "comment": "Array of predictions."},
                {"name": "predict_proba", "dtype": "array", "comment": "Array of class probabilities."},
                {"name": "max_leaf_nodes", "dtype": "int", "comment": "The maximum number of leaf nodes in best-first fashion."}
            ]
        },
        {
            "name": "RandomForestRegressor",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "comment": "The number of trees in the forest."},
                {"name": "max_depth", "default": "None", "dtype": "int", "comment": "The maximum depth of the tree."},
                {"name": "criterion", "default": "mse", "dtype": "str", "comment": "The function to measure the quality of a split."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node."},
                {"name": "min_weight_fraction_leaf", "default":"0.0", "dtype": "float", "comment": "The minimum weighted fraction of the sum total of weights required to be at a leaf node."},
                {"name": "max_features", "default": "1.0", "dtype": "int or float", "comment": "The number of features to consider when looking for the best split."},
                {"name": "max_leaf_nodes", "default": "None", "dtype": "int", "comment": "Grow trees with max_leaf_nodes in best-first fashion."},
                {"name": "min_impurity_decrease", "default": "0.0", "dtype": "float", "comment": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},
                {"name": "bootstrap", "default": "True", "dtype": "bool", "comment": "Whether bootstrap samples are used when building trees."},
                {"name": "oob_score", "default": "False", "dtype": "bool", "comment": "Whether to use out-of-bag samples to estimate the generalization score."},
                {"name": "n_jobs", "default": "-1", "dtype": "int", "comment": "The number of jobs to run in parallel."},
                {"name": "random_state", "default": "None", "dtype": "int", "comment": "Controls both the randomness of the bootstrapping of the samples and the sampling of the features."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "Controls the verbosity when fitting and predicting."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "comment": "Complexity parameter used for Minimal Cost-Complexity Pruning."},
                {"name": "max_samples", "default": "None", "dtype": "int", "comment": "If bootstrap is True, the number of samples to draw from X to train each base estimator."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "comment": "The predicted values for each sample."},
                {"name": "predict_proba", "dtype": "array", "comment": "The class probabilities of the input samples."}
            ]
        },
        {
            "name": "KNeighborsRegressor",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "n_neighbors", "default": "5", "dtype": "int", "comment": "Number of neighbors to use by default for kneighbors queries."},
                {"name": "weights", "default": "uniform", "dtype": "str", "comment": "Weight function used in prediction. Possible values: 'uniform' or 'distance'."},
                {"name": "algorithm", "default": "auto", "dtype": "str", "comment": "Algorithm used to compute the nearest neighbors. Possible values: 'auto', 'ball_tree', 'kd_tree', 'brute'."},
                {"name": "leaf_size", "default": "30", "dtype": "int", "comment": "Leaf size passed to BallTree or KDTree."},
                {"name": "p", "default": "2", "dtype": "float", "comment": "Power parameter for the Minkowski metric. When p = 1, equivalent to using Manhattan distance, and Euclidean distance for p = 2."},
                {"name": "metric", "default": "minkowski", "dtype": "str or callable", "comment": "Metric to use for distance computation."},
                {"name": "metric_params", "default": "None", "dtype": "dict", "comment": "Additional keyword arguments for the metric function."},
                {"name": "n_jobs", "default": "-1", "dtype": "int", "comment": "The number of parallel jobs to run for neighbors search."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "comment": "The predicted values for each sample."}
            ]
        },
        {
            "name": "GradientBoostingRegressor",
            "task": "Regression",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "comment": "The number of boosting stages to perform. Values must be in the range [1, inf)."},
                {"name": "learning_rate", "default": "0.1", "dtype": "float", "comment": "Learning rate shrinks the contribution of each tree by learning_rate. Values must be in the range [0.0, inf)."},
                {"name": "max_depth", "default": "3", "dtype": "int or None", "comment": "Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If int, values must be in the range [1, inf)."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node. If int, values must be in the range [2, inf). If float, values must be in the range (0.0, 1.0] and min_samples_split will be ceil(min_samples_split * n_samples)."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0) and min_samples_leaf will be ceil(min_samples_leaf * n_samples)."},
                {"name": "max_features", "default": "None", "dtype": "int, float, or {'auto', 'sqrt', 'log2'}", "comment": "The number of features to consider when looking for the best split. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0] and the features considered at each split will be max(1, int(max_features * n_features_in_)). If 'sqrt', then max_features=sqrt(n_features). If 'log2', then max_features=log2(n_features). If None, then max_features=n_features."},
                {"name": "subsample", "default": "1.0", "dtype": "float", "comment": "The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. Values must be in the range (0.0, 1.0]."},
                {"name": "criterion", "default": "friedman_mse", "dtype": "{'friedman_mse', 'squared_error'}", "comment": "The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases. New in version 0.18."},
                {"name": "min_weight_fraction_leaf", "default": "0.0", "dtype": "float", "comment": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. Values must be in the range [0.0, 0.5]."},
                {"name": "init", "default": "None", "dtype": "estimator or 'zero'", "comment": "An estimator object that is used to compute the initial predictions. init has to provide fit and predict. If 'zero', the initial raw predictions are set to zero. By default a DummyEstimator is used, predicting either the average target value (for loss='squared_error'), or a quantile for the other losses."},
                {"name": "random_state", "default": "None", "dtype": "int, RandomState instance or None", "comment": "Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split. It also controls the random splitting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls. See Glossary."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "Enable verbose output. If 1 then it prints progress and performance once in a while. If greater than 1 then it prints progress and performance for every tree. Values must be in the range [0, inf)."},
                {"name": "max_leaf_nodes", "default": "None", "dtype": "int or None", "comment": "Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. Values must be in the range [2, inf). If None, then unlimited number of leaf nodes."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See the Glossary."},
                {"name": "validation_fraction", "default": "0.1", "dtype": "float", "comment": "The proportion of training data to set aside as validation set for early stopping. Values must be in the range (0.0, 1.0). Only used if n_iter_no_change is set to an integer. New in version 0.20."},
                {"name": "n_iter_no_change", "default": "None", "dtype": "int", "comment": "n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default, it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations. The split is stratified. Values must be in the range [1, inf). New in version 0.20."},
                {"name": "tol", "default": "0.0001", "dtype": "float", "comment": "Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations, the training stops. Values must be in the range [0.0, inf). New in version 0.20."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "comment": "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. Values must be in the range [0.0, inf). See Minimal Cost-Complexity Pruning for details. New in version 0.22."},
                {"name": "loss", "default": "squared_error", "dtype": "str", "comment": "Loss function to be optimized. 'squared_error' refers to the squared error for regression. 'absolute_error' refers to the absolute error of regression and is a robust loss function. 'huber' is a combination of the two. 'quantile' allows quantile regression (use alpha to specify the quantile)."}
            ],
            "return": [
                {"name": "estimators_", "dtype": "array of objects"},
                {"name": "feature_importances_", "dtype": "ndarray of shape (n_features,)"},
                {"name": "oob_improvement_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "oob_scores_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "oob_score_", "dtype": "float"},
                {"name": "train_score_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "init_estimator_", "dtype": "estimator"},
                {"name": "estimators_", "dtype": "ndarray of DecisionTreeRegressor of shape (n_estimators, 1)"},
                {"name": "n_estimators_", "dtype": "int"},
                {"name": "n_features_in_", "dtype": "int"},
                {"name": "feature_names_in_", "dtype": "ndarray of shape (n_features_in_,)"},
                {"name": "max_features_", "dtype": "int"}
            ]
        },
        {
            "name": "LogisticRegression",
            "task": "Classification",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "C", "default": "1.0", "dtype": "float", "comment": "Inverse of regularization strength; must be a positive float. Smaller values specify stronger regularization."},
                {"name": "penalty", "default": "l2", "dtype": "str", "comment": "Specify the norm of the penalty. 'l2': L2 penalty term (default), 'l1': L1 penalty term, 'elasticnet': both L1 and L2 penalty terms. 'None': no penalty added. Deprecated since version 1.2; use None instead."},
                {"name": "dual", "default": "False", "dtype": "bool"},
                {"name": "tol", "default": "0.0001", "dtype": "float"},
                {"name": "intercept_scaling", "default": "1", "dtype": "float"},
                {"name": "class_weight", "default": "None", "dtype": "dict or 'balanced'"},
                {"name": "fit_intercept", "default": "True", "dtype": "bool", "comment": "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function."},
                {"name": "random_state", "default": "None", "dtype": "int or RandomState instance"},
                {"name": "solver", "default": "lbfgs", "dtype": "str"},
                {"name": "max_iter", "default": "100", "dtype": "int", "comment": "Maximum number of iterations taken for the solvers to converge."},
                {"name": "multi_class", "default": "'auto'", "dtype": "'auto', 'ovr', 'multinomial'", "comment": "If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimized is the multinomial loss fit across the entire probability distribution."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "For the liblinear and lbfgs solvers set verbose to any positive number for verbosity."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit as initialization."},
                {"name": "n_jobs", "default": "-1", "dtype": "int", "comment": "Number of CPU cores used when parallelizing over classes if multi_class='ovr'."},
                {"name": "l1_ratio", "default": "None", "dtype": "float", "comment": "The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if penalty='elasticnet'."}
                
            ],
            "return": [
                {"name": "coef_", "dtype": "array", "comment": "Coefficient of the features."},
                {"name": "intercept_", "dtype": "float", "comment": "Intercept (bias) added to the decision function."}
            ]
        },
        {
            "name": "SVC",
            "task": "Classification",
            "data": "Supervised, numerical data",
            "params": [
                {"name": "C", "default": "1.0", "dtype": "float", "comment": "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared L2 penalty."},
                {"name": "kernel", "default": "rbf", "dtype": "str", "comment": "Specifies the kernel type to be used in the algorithm. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices."},
                {"name": "gamma", "default": "scale", "dtype": "str", "comment": "Kernel coefficient for ‘rbf’, ‘poly’, and ‘sigmoid’. If gamma='scale' (default) is passed, then it uses 1 / (n_features * X.var()) as the value of gamma."},
                {"name": "degree", "default":"3", "dtype": "int", "comment": "Degree of the polynomial kernel function (‘poly’). Must be non-negative. Ignored by all other kernels."},
                {"name": "coef0", "default": "0.0", "dtype": "float", "comment": "Independent term in the kernel function. It is only significant in ‘poly’ and ‘sigmoid’."},
                {"name": "shrinking", "default": "True", "dtype": "bool", "comment": "Whether to use the shrinking heuristic."},
                {"name": "probability", "default": "False", "dtype": "bool", "comment": "Whether to enable probability estimates. This must be enabled prior to calling fit, will slow down that method as it internally uses 5-fold cross-validation."},
                {"name": "tol", "default": "0.001", "dtype": "float", "comment": "Tolerance for stopping criterion."},
                {"name": "cache_size", "default": "200", "dtype": "float", "comment": "Specify the size of the kernel cache (in MB)."},
                {"name": "class_weight", "default": "None", "dtype": "dict or ‘balanced’", "comment": "Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one."},
                {"name": "verbose", "default": "False", "dtype": "bool", "comment": "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm."},
                {"name": "max_iter", "default": "-1", "dtype": "int", "comment": "Hard limit on iterations within solver, or -1 for no limit."},
                {"name": "decision_function_shape", "default": "ovr", "dtype": "str ", "comment": "Whether to return a one-vs-rest (‘ovr’) decision function or the original one-vs-one (‘ovo’) decision function."}
            ],
            "return": [
                {"name": "support_", "dtype": "array", "comment": "Indices of support vectors."},
                {"name": "dual_coef_", "dtype": "array", "comment": "Dual coefficients of the support vector in the decision function, multiplied by their targets."},
                {"name": "intercept_", "dtype": "array", "comment": "Constants in the decision function."}
            ]
        },
        {
            "name": "DecisionTreeClassifier",
            "task": "Classification",
            "data": "Supervised, binary data",
            "params": [
                {"name": "criterion", "default": "gini", "dtype": "str", "description": "The function to measure the quality of a split."},
                {"name": "splitter", "default": "best", "dtype": "str", "description": "The strategy used to choose the split at each node."},
                {"name": "max_depth", "default": "None", "dtype": "int", "description": "The maximum depth of the tree."},
                {"name": "min_samples_split", "default": "2", "dtype": "int", "description": "The minimum number of samples required to split an internal node."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "description": "The minimum number of samples required to be at a leaf node."},
                {"name": "min_weight_fraction_leaf", "default": "0.0", "dtype": "float", "description": "The minimum weighted fraction of the sum total of weights required to be at a leaf node."},
                {"name": "max_features", "default": "None", "dtype": "int or float", "description": "The number of features to consider when looking for the best split."},
                {"name": "random_state", "default": "None", "dtype": "int", "description": "Controls the randomness of the estimator."},
                {"name": "max_leaf_nodes", "default": "None", "dtype": "int", "description": "Grow a tree with max_leaf_nodes in best-first fashion."},
                {"name": "min_impurity_decrease", "default": "0.0", "dtype": "float", "description": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},
                {"name": "class_weight", "default": "None", "dtype": "dict, list of dict, or 'balanced'", "description": "Weights associated with classes."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "description": "Complexity parameter for Minimal Cost-Complexity Pruning."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "description": "Predicted class labels for each data point."},
                {"name": "predict_proba", "dtype": "array", "description": "Probability estimates of the classes."},
                {"name": "max_leaf_nodes", "dtype": "int", "description": "Inferred value of max_leaf_nodes."}
            ]
        },
        {
            "name": "RandomForestClassifier",
            "task": "Classification",
            "data": "Supervised, binary data",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "comment": "The number of trees in the forest."},
                {"name": "max_depth", "default": "None", "dtype": "int", "comment": "The maximum depth of the tree."},
                {"name": "criterion", "default": "gini", "dtype": "str", "comment": "The function to measure the quality of a split."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node."},
                {"name": "min_weight_fraction_leaf", "default": "0.0", "dtype": "float", "comment": "The minimum weighted fraction of the sum total of weights required to be at a leaf node."},
                {"name": "max_features", "default": "sqrt", "dtype": "int or float", "comment": "The number of features to consider when looking for the best split."},
                {"name": "max_leaf_nodes", "default": "None", "dtype": "int", "comment": "Grow trees with max_leaf_nodes in best-first fashion."},
                {"name": "min_impurity_decrease", "default": "0.0", "dtype": "float", "comment": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},
                {"name": "bootstrap", "default": "True", "dtype": "bool", "comment": "Whether bootstrap samples are used when building trees."},
                {"name": "oob_score", "default": "False", "dtype": "bool", "comment": "Whether to use out-of-bag samples to estimate the generalization score."},
                {"name": "n_jobs", "default": "-1", "dtype": "int", "comment": "The number of jobs to run in parallel."},
                {"name": "random_state", "default": "None", "dtype": "int", "comment": "Controls both the randomness of the bootstrapping of the samples and the sampling of the features."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "Controls the verbosity when fitting and predicting."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble."},
                {"name": "class_weight", "default": "None", "dtype": "dict", "comment": "Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "comment": "Complexity parameter used for Minimal Cost-Complexity Pruning."},
                {"name": "max_samples", "default": "None", "dtype": "int", "comment": "If bootstrap is True, the number of samples to draw from X to train each base estimator."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "comment": "The predicted values for each sample."},
                {"name": "predict_proba", "dtype": "array", "comment": "The class probabilities of the input samples."}
            ]
        },
        {
            "name": "KNeighborsClassifier",
            "task": "Classification",
            "data": "Supervised, binary data",
            "params": [
                {"name": "n_neighbors", "default": "5", "dtype": "int", "comment": "Number of neighbors to use by default for kneighbors queries."},
                {"name": "weights", "default": "uniform", "dtype": "str", "comment": "Weight function used in prediction. Possible values: 'uniform' or 'distance'."},
                {"name": "algorithm", "default": "auto", "dtype": "str", "comment": "Algorithm used to compute the nearest neighbors. Possible values: 'auto', 'ball_tree', 'kd_tree', 'brute'."},
                {"name": "leaf_size", "default": "30", "dtype": "int", "comment": "Leaf size passed to BallTree or KDTree."},
                {"name": "p", "default": "2", "dtype": "float", "comment": "Power parameter for the Minkowski metric. When p = 1, equivalent to using Manhattan distance, and Euclidean distance for p = 2."},
                {"name": "metric", "default": "minkowski", "dtype": "str", "comment": "Metric to use for distance computation."},
                {"name": "metric_params", "default": "None", "dtype": "dict", "comment": "Additional keyword arguments for the metric function."},
                {"name": "n_jobs", "default": "-1", "dtype": "int", "comment": "The number of parallel jobs to run for neighbors search."}
            ],
            "return": [
                {"name": "predict", "dtype": "array", "comment": "The predicted values for each sample."},
                {"name": "predict_proba", "dtype": "array", "comment": "Class probability estimates for the input samples."}
            ]
        },
        {
            "name": "GradientBoostingClassifier",
            "task": "Classification",
            "data": "Supervised, binary or multiclass data",
            "params": [
                {"name": "n_estimators", "default": "100", "dtype": "int", "comment": "The number of boosting stages to perform. Values must be in the range [1, inf)."},
                {"name": "learning_rate", "default": "0.1", "dtype": "float", "comment": "Learning rate shrinks the contribution of each tree by learning_rate. Values must be in the range [0.0, inf)."},
                {"name": "max_depth", "default": "3", "dtype": "int or None", "comment": "Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If int, values must be in the range [1, inf)."},
                {"name": "min_samples_split", "default": "2", "dtype": "int or float", "comment": "The minimum number of samples required to split an internal node. If int, values must be in the range [2, inf). If float, values must be in the range (0.0, 1.0] and min_samples_split will be ceil(min_samples_split * n_samples)."},
                {"name": "min_samples_leaf", "default": "1", "dtype": "int or float", "comment": "The minimum number of samples required to be at a leaf node. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0) and min_samples_leaf will be ceil(min_samples_leaf * n_samples)."},
                {"name": "max_features", "default": "None", "dtype": "int, float, or {'auto', 'sqrt', 'log2'}", "comment": "The number of features to consider when looking for the best split. If int, values must be in the range [1, inf). If float, values must be in the range (0.0, 1.0] and the features considered at each split will be max(1, int(max_features * n_features_in_)). If 'sqrt', then max_features=sqrt(n_features). If 'log2', then max_features=log2(n_features). If None, then max_features=n_features."},
                {"name": "subsample", "default": "1.0", "dtype": "float", "comment": "The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. Values must be in the range (0.0, 1.0]."},
                {"name": "criterion", "default": "friedman_mse", "dtype": "{'friedman_mse', 'squared_error'}", "comment": "The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases. New in version 0.18."},
                {"name": "min_weight_fraction_leaf", "default": "0.0", "dtype": "float", "comment": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. Values must be in the range [0.0, 0.5]."},
                {"name": "init", "default": "None", "dtype": "object or 'zero'", "comment": "An estimator object that is used to compute the initial predictions. init has to provide fit and predict_proba. If 'zero', the initial raw predictions are set to zero. By default, a DummyEstimator predicting the classes priors is used."},
                {"name": "random_state", "default": "None", "dtype": "int, RandomState instance or None", "comment": "Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split. It also controls the random splitting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls. See Glossary."},
                {"name": "verbose", "default": "0", "dtype": "int", "comment": "Enable verbose output. If 1 then it prints progress and performance once in a while. If greater than 1 then it prints progress and performance for every tree. Values must be in the range [0, inf)."},
                {"name": "max_leaf_nodes", "default": "None", "dtype": "int or None", "comment": "Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. Values must be in the range [2, inf). If None, then unlimited number of leaf nodes."},
                {"name": "warm_start", "default": "False", "dtype": "bool", "comment": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See the Glossary."},
                {"name": "validation_fraction", "default": "0.1", "dtype": "float", "comment": "The proportion of training data to set aside as validation set for early stopping. Values must be in the range (0.0, 1.0). Only used if n_iter_no_change is set to an integer. New in version 0.20."},
                {"name": "n_iter_no_change", "default": "None", "dtype": "int", "comment": "n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default, it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations. The split is stratified. Values must be in the range [1, inf). New in version 0.20."},
                {"name": "tol", "default": "0.0001", "dtype": "float", "comment": "Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations, the training stops. Values must be in the range [0.0, inf). New in version 0.20."},
                {"name": "ccp_alpha", "default": "0.0", "dtype": "non-negative float", "comment": "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. Values must be in the range [0.0, inf). See Minimal Cost-Complexity Pruning for details. New in version 0.22."},
                {"name": "loss", "default": "log_loss", "dtype": "str", "comment": "The loss function to be optimized. 'log_loss' refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss 'exponential', gradient boosting recovers the AdaBoost algorithm."}
            ],
            "return": [
                {"name": "estimators_", "dtype": "array of objects"},
                {"name": "feature_importances_", "dtype": "ndarray of shape (n_features,)"},
                {"name": "oob_improvement_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "oob_scores_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "oob_score_", "dtype": "float"},
                {"name": "train_score_", "dtype": "ndarray of shape (n_estimators,)"},
                {"name": "init_estimator_", "dtype": "object"},
                {"name": "estimators_", "dtype": "ndarray of DecisionTreeRegressor of shape (n_estimators, loss_.K)"},
                {"name": "classes_", "dtype": "ndarray of shape (n_classes,)"},
                {"name": "n_features_in_", "dtype": "int"},
                {"name": "feature_names_in_", "dtype": "ndarray of shape (n_features_in_,)"},
                {"name": "n_classes_", "dtype": "int"},
                {"name": "max_features_", "dtype": "int"}
            ]
        },
        {
            "name": "GaussianNB",
            "task": "Classification",
            "data": "Supervised, binary or multiclass data",
            "params": [
                {"name": "priors", "default": "None", "dtype": "array-like of shape (n_classes,)", "comment": "Prior probabilities of the classes. If specified, the priors are not adjusted according to the data."},
                {"name": "var_smoothing", "default": "0.000000001", "dtype": "float", "comment": "Portion of the largest variance of all features that is added to variances for calculation stability. New in version 0.20."}
            ],
            "return": [
                {"name": "class_log_prior_", "dtype": "array", "comment": "Log prior probabilities of classes."},
                {"name": "class_count_", "dtype": "array", "comment": "Number of training samples observed in each class."},
                {"name": "feature_count_", "dtype": "array", "comment": "Sum of feature occurrences over all classes."}
            ]
        }        
    ]
}